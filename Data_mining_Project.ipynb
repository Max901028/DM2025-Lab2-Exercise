{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6476b73",
   "metadata": {},
   "source": [
    "# Using machine learning for sentiment analysis\n",
    "The goal of this project is to train a Model for Text Sentiment Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0dc88",
   "metadata": {},
   "source": [
    "## Data Preprocess & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae15a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_type': 'post', '_source': {'post': {'post_id': '0x61fc95', 'text': 'We got the ranch, loaded our guns and sat up till sunrise.', 'hashtags': []}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_json('dm-lab-2-private-competition/final_posts.json')\n",
    "\n",
    "print(df_raw.iloc[0,0])\n",
    "df = pd.DataFrame()\n",
    "df[\"post_id\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"post_id\"])\n",
    "df[\"text\"]    = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"text\"])\n",
    "df[\"hashtags\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844aed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  emotion  ident  \n",
       "0                                     []      NaN   test  \n",
       "1                                     []      joy  train  \n",
       "2                                     []     fear  train  \n",
       "3                                     []      joy  train  \n",
       "4                                     []      NaN   test  \n",
       "...                                  ...      ...    ...  \n",
       "64166                                 []    anger  train  \n",
       "64167                                 []      joy  train  \n",
       "64168                                 []      NaN   test  \n",
       "64169  [texans, astros, sadness, losers]  sadness  train  \n",
       "64170                                 []      NaN   test  \n",
       "\n",
       "[64171 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ident = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "emotion['post_id'] = emotion['id']\n",
    "df = df.merge(emotion[['post_id', 'emotion']], on='post_id', how='left')\n",
    "df['ident'] = ident['split']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7b977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>Thank you so much❤️</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>Stinks because ive been in this program for a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64164</th>\n",
       "      <td>0xd740f2</td>\n",
       "      <td>why is everybody seem sp serious?</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64165</th>\n",
       "      <td>0x99267e</td>\n",
       "      <td>You can cross fuck off, its 10f all winter in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47890 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text  \\\n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "7      0x2ffb63                                Thank you so much❤️   \n",
       "9      0x989146  Stinks because ive been in this program for a ...   \n",
       "...         ...                                                ...   \n",
       "64164  0xd740f2                  why is everybody seem sp serious?   \n",
       "64165  0x99267e  You can cross fuck off, its 10f all winter in ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "\n",
       "                                hashtags  emotion  ident  \n",
       "1                                     []      joy  train  \n",
       "2                                     []     fear  train  \n",
       "3                                     []      joy  train  \n",
       "7                                     []      joy  train  \n",
       "9                                     []      joy  train  \n",
       "...                                  ...      ...    ...  \n",
       "64164                                 []      joy  train  \n",
       "64165                                 []    anger  train  \n",
       "64166                                 []    anger  train  \n",
       "64167                                 []      joy  train  \n",
       "64169  [texans, astros, sadness, losers]  sadness  train  \n",
       "\n",
       "[47890 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['ident'] == 'train']\n",
    "test_df =  df[df['ident'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff936f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>Thank you so much❤️</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>Stinks because ive been in this program for a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id                                               text hashtags  \\\n",
       "1  0x35663e  I bet there is an army of married couples who ...       []   \n",
       "2  0xc78afe                         This could only end badly.       []   \n",
       "3  0x90089c  My sister squeezed a lime in her milk when she...       []   \n",
       "7  0x2ffb63                                Thank you so much❤️       []   \n",
       "9  0x989146  Stinks because ive been in this program for a ...       []   \n",
       "\n",
       "  emotion  ident  \n",
       "1     joy  train  \n",
       "2    fear  train  \n",
       "3     joy  train  \n",
       "7     joy  train  \n",
       "9     joy  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c367dd",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Text Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. \n",
    "\n",
    "It transforms text into a more digestible form so that deep learning algorithms can perform better.\n",
    "\n",
    "The Preprocessing steps taken are:\n",
    "\n",
    "1. Lower Casing: Each text is converted to lowercase.\n",
    "\n",
    "2. Replacing URLs: Links starting with 'http' or 'https' or 'www' are replaced by '<url>'.\n",
    "\n",
    "3. Replacing Usernames: Replace @Usernames with word '<user>'. [eg: '@Kaggle' to '<user>'].\n",
    "\n",
    "4. Replacing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n",
    "\n",
    "5. Replacing Emojis: Replace emojis by using a regex expression. [eg: ':)' to '<smile>']\n",
    "\n",
    "6. Replacing Contractions: Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n",
    "\n",
    "7. Removing Non-Alphabets: Replacing characters except Digits, Alphabets and pre-defined Symbols with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d57ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/2270860636.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['processed_text'] = train_df.text.apply(preprocess_apply)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "contractions = pd.read_csv('dm-lab-2-private-competition/contractions.csv.xls', index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# Defining regex for emojis\n",
    "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
    "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
    "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    # Replace all emojis.\n",
    "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
    "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
    "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
    "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
    "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
    "\n",
    "    for contraction, replacement in contractions_dict.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "\n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
    "\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "train_df['processed_text'] = train_df.text.apply(preprocess_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cc3595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I bet there is an army of married couples who did the same exact thing.\n",
      "processed_text: joy \n",
      "\n",
      "Text: This could only end badly.\n",
      "processed_text: fear \n",
      "\n",
      "Text: My sister squeezed a lime in her milk when she was 12. Same thing happened, but we told her it would happen AFTER she did it ..\n",
      "processed_text: joy \n",
      "\n",
      "Text: Thank you so much❤️\n",
      "processed_text: joy \n",
      "\n",
      "Text: Stinks because ive been in this program for a year with no pay.....back to the drawing board.\n",
      "processed_text: joy \n",
      "\n",
      "Text: The overall response is try and empower women, abolish prostitution and stop giving lazy men money because they want to live out their idiotic fantasy lives. \n",
      "processed_text: anger \n",
      "\n",
      "Text: Your market sucks\n",
      "processed_text: anger \n",
      "\n",
      "Text: here’s hoping the same is true for me!\n",
      "processed_text: joy \n",
      "\n",
      "Text: She looks like a televangelist.\n",
      "processed_text: joy \n",
      "\n",
      "Text: Rap that will Cut other raper's throat. Who said that? @Paedeezy #badd #wicked. #bright city lights\n",
      "processed_text: anger \n",
      "\n",
      "Text: She’s a good person who stands up for people not like her, and they can’t stand that.\n",
      "processed_text: joy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the texts now turn into this\n",
    "count=0\n",
    "for row in train_df.itertuples():\n",
    "    print(\"Text:\", row[2])\n",
    "    print('processed_text:', row[4],\"\\n\")\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26194f46",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e9a707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45495\n",
      "Test size : 2395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "\n",
    "\n",
    "X_data = train_df[\"processed_text\"].astype(str).values\n",
    "y_data = train_df[\"emotion\"].values\n",
    "\n",
    "# 切 train / test（這裡 5% 當 test）\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data  # 依照情緒比例分層抽樣\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dd0638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 300\n",
      "X_train shape: (45495, 300)\n",
      "X_test shape : (2395, 300)\n"
     ]
    }
   ],
   "source": [
    "# 載入 Pretrained Google_news Word2Vec 模型\n",
    "\n",
    "w2v_path = \"dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "embedding_dim = w2v.vector_size  \n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    \"\"\"把一則文字轉成平均的 Word2Vec 向量\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # 如果裡面沒有任何在詞向量中的字，就給 0 向量\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "#  將文字轉成向量 \n",
    "\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_train, embedding_dim)\n",
    "print(\"X_test shape :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "num_classes: 6\n"
     ]
    }
   ],
   "source": [
    "# Python can't understand emotions like \"sadness\" or \"joy\", so here we have to change all 6 emotions into a numeric category of 0~6\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc9e9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m77,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\"),  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "rlr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0e09e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - accuracy: 0.6039 - loss: 1.0566 - val_accuracy: 0.5888 - val_loss: 1.0786 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6100 - loss: 1.0369 - val_accuracy: 0.5895 - val_loss: 1.0857 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754us/step - accuracy: 0.6174 - loss: 1.0176 - val_accuracy: 0.5943 - val_loss: 1.0738 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.6253 - loss: 0.9958 - val_accuracy: 0.5949 - val_loss: 1.0734 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.6328 - loss: 0.9709 - val_accuracy: 0.5921 - val_loss: 1.0796 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1223/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.6446 - loss: 0.9348\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.6434 - loss: 0.9470 - val_accuracy: 0.5947 - val_loss: 1.0848 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - accuracy: 0.6618 - loss: 0.8963 - val_accuracy: 0.5967 - val_loss: 1.1068 - learning_rate: 5.0000e-04\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - accuracy: 0.6117 - loss: 1.0273\n",
      "Test loss: 1.0273  |  Test acc: 0.6117\n"
     ]
    }
   ],
   "source": [
    "# Input training data\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=[es, rlr],\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137cc69",
   "metadata": {},
   "source": [
    "##  Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d05d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/3121004903.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we got the ranch  loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>and that got my head bobbing a little bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>same  glad it is not just out store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>like always i will wait and see thanks for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>thereis a bit of room between  not loving sub ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we all do it sometimes do not worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>this new year i visited more relatives than us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>r u a dad or did ur dad leave u both have bad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>pre prepare direction plays hale and hearty si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text hashtags  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...       []   \n",
       "4      0xaba820         and that got my head bobbing a little bit.       []   \n",
       "5      0x66e44d                Same. Glad it's not just out store.       []   \n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...       []   \n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...       []   \n",
       "...         ...                                                ...      ...   \n",
       "64146  0x0f273c                We all do it sometimes don't worry.       []   \n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...       []   \n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...       []   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...       []   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...       []   \n",
       "\n",
       "      emotion ident                                     processed_text  \n",
       "0         NaN  test  we got the ranch  loaded our guns and sat up t...  \n",
       "4         NaN  test         and that got my head bobbing a little bit   \n",
       "5         NaN  test               same  glad it is not just out store   \n",
       "6         NaN  test  like always i will wait and see thanks for the...  \n",
       "8         NaN  test  thereis a bit of room between  not loving sub ...  \n",
       "...       ...   ...                                                ...  \n",
       "64146     NaN  test               we all do it sometimes do not worry   \n",
       "64150     NaN  test  this new year i visited more relatives than us...  \n",
       "64157     NaN  test  r u a dad or did ur dad leave u both have bad ...  \n",
       "64168     NaN  test  i got my first raspberry from a crowd surfer f...  \n",
       "64170     NaN  test  pre prepare direction plays hale and hearty si...  \n",
       "\n",
       "[16281 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unclassified text data\n",
    "test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6350f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.22979933e-01, 3.68012721e-03, 2.64608636e-02, 5.12456417e-01,\n",
       "        7.52074877e-03, 2.26901829e-01],\n",
       "       [5.79092763e-02, 5.67220151e-03, 6.81349695e-01, 1.44356295e-01,\n",
       "        3.10226120e-02, 7.96900317e-02],\n",
       "       [9.01271924e-02, 8.57952144e-03, 7.43259070e-03, 6.33115232e-01,\n",
       "        1.22691981e-01, 1.38053477e-01],\n",
       "       ...,\n",
       "       [2.22188517e-01, 3.17916125e-02, 8.08604062e-03, 4.45347011e-01,\n",
       "        2.19614059e-01, 7.29727298e-02],\n",
       "       [3.13539892e-01, 1.63444970e-02, 3.00939441e-01, 3.13508034e-01,\n",
       "        4.22553048e-02, 1.34128630e-02],\n",
       "       [1.01556545e-02, 1.40085549e-05, 1.01512715e-05, 9.83254015e-01,\n",
       "        9.00041414e-05, 6.47614058e-03]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.vstack([text_to_vec(t) for t in test_df['processed_text']])\n",
    "pred_result= model.predict(predict)\n",
    "pred_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0705fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16281, 6)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row has 6 numbers representing possible emotions. \n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "pred_result.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e50a7f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text_id                                               text emotion\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...     joy\n",
       "4      0xaba820         and that got my head bobbing a little bit.    fear\n",
       "5      0x66e44d                Same. Glad it's not just out store.     joy\n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...     joy\n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...     joy\n",
       "...         ...                                                ...     ...\n",
       "64146  0x0f273c                We all do it sometimes don't worry.     joy\n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...   anger\n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...     joy\n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   anger\n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...     joy\n",
       "\n",
       "[16281 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class_idx = np.argmax(pred_result, axis=1) # For each text choose the most likely emotion\n",
    "pred_labels = label_encoder.inverse_transform(pred_class_idx) # Changing the numeric emotion category back to words.\n",
    "result_df = pd.DataFrame({\"Text_id\" : test_df[\"post_id\"],\"text\": test_df[\"text\"], \"emotion\" :pred_labels})\n",
    "result_df\n",
    "# From the train we learnt that this model has a 62% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84fd384f",
   "metadata": {},
   "source": [
    "# Other tries durin DM2025 kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5056e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_json('dm-lab-2-private-competition/final_posts.json')\n",
    "\n",
    "print(df_raw.iloc[0,0])\n",
    "df = pd.DataFrame()\n",
    "df[\"post_id\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"post_id\"])\n",
    "df[\"text\"]    = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"text\"])\n",
    "df[\"hashtags\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"hashtags\"])\n",
    "#df.text = df.text.str.lower()\n",
    "#df_dict = df.to_dict()['text']\n",
    "##### contraction data\n",
    "import re\n",
    "contractions = pd.read_csv('/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/contractions.csv.xls', index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# Defining regex for emojis\n",
    "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
    "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
    "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    # Replace all emojis.\n",
    "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
    "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
    "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
    "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
    "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
    "\n",
    "    for contraction, replacement in contractions_dict.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "\n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
    "\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "df['processed_text'] = df.text.apply(preprocess_apply)\n",
    "df\n",
    "count=0\n",
    "for row in df.itertuples():\n",
    "    print(\"Text:\", row[2])\n",
    "    print('processed_text:', row[4],\"\\n\")\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "ident = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "emotion['post_id'] = emotion['id']\n",
    "df = df.merge(emotion[['post_id', 'emotion']], on='post_id', how='left')\n",
    "df['ident'] = ident['split']\n",
    "df\n",
    "df.isnull().value_counts()\n",
    "train_df = df[df['ident'] == 'train']\n",
    "test_df =  df[df['ident'] == 'test']\n",
    "train_df[\"emotion\"].value_counts()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.show()\n",
    "### Dealing with skewed Data\n",
    "min_n = train_df[\"emotion\"].value_counts().min()\n",
    "\n",
    "balanced_df = (\n",
    "    train_df.groupby(\"emotion\")\n",
    "      .sample(n=min_n, random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "balanced_df[\"emotion\"].value_counts()\n",
    "train_df = df[df['ident'] == 'train']\n",
    "test_df =  df[df['ident'] == 'test']\n",
    "\n",
    "frac_dict = {\n",
    "    \"anger\": 0.8,\n",
    "    \"joy\": 0.5,\n",
    "    \"sadness\": 1,\n",
    "    \"fear\": 1,\n",
    "    \"disgust\":1,\n",
    "    \"surprise\":1\n",
    "}\n",
    "\n",
    "\n",
    "transformed_df = (\n",
    "    df.groupby(\"emotion\")\n",
    "      .apply(lambda g: g.sample(frac=frac_dict[g.name], random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "transformed_df.sample(frac=1)\n",
    "transformed_df[\"emotion\"].value_counts()\n",
    "## 2. Feature Engineering Steps\n",
    "## AI 法\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "env_path = \"./config/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "env_path = \"./config/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# System instruction that can dictate how the model behaves in the output, can be customized as needed\n",
    "SYSTEM_INSTRUCTION = (\n",
    "        \"You are a text emotion classifying machine. You assign each text an emotion\"\n",
    "    )\n",
    "\n",
    "# Max amount of tokens that the model can output, the Gemini 2.5 Models have this maximum amount\n",
    "# For other models need to check their documentation \n",
    "MAX_OUTPUT_TOKENS = 65535\n",
    "MODEL_NAME = \"gemini-2.0-flash-lite\" # Other models: \"gemini-2.5-pro\", \"gemini-2.5-flash\"; Check different max output tokens: \"gemini-2.0-flash\" , \"gemini-2.0-flash-lite\" \n",
    "\n",
    "# We disable the safety settings, as no moderation is needed in our tasks\n",
    "SAFETY_SETTINGS = [\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\")\n",
    "]\n",
    "\n",
    "#IMPORTANT: The script loads your API key from a `.env` file located in the `./config/` directory. \n",
    "# You must create this file and add your API key like this: `GOOGLE_API_KEY='YOUR_API_KEY_HERE'`\n",
    "\n",
    "# We input the API Key to be able to use the Gemini models\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# We also set LangExtract to use the API key as well:\n",
    "if 'GEMINI_API_KEY' not in os.environ:\n",
    "    os.environ['GEMINI_API_KEY'] = api_key\n",
    "\n",
    "def prompt_gemini(\n",
    "        input_prompt: list,\n",
    "        schema = None,\n",
    "        temperature: float = 0.0,\n",
    "        system_instruction: str = SYSTEM_INSTRUCTION,\n",
    "        max_output_tokens: int = MAX_OUTPUT_TOKENS,\n",
    "        client: genai.Client = client,\n",
    "        model_name: str = MODEL_NAME,\n",
    "        new_config: types.GenerateContentConfig = None,\n",
    "        with_tools: bool = False,\n",
    "        with_parts: bool = False,\n",
    "        with_tokens_info: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            # If we need a JSON schema we set up the following\n",
    "            if schema:\n",
    "                generate_content_config = types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    system_instruction=system_instruction,\n",
    "                    max_output_tokens=max_output_tokens,\n",
    "                    response_modalities=[\"TEXT\"],\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=schema,\n",
    "                    safety_settings=SAFETY_SETTINGS\n",
    "                )\n",
    "            # If there is no need we leave it unstructured\n",
    "            else:\n",
    "                generate_content_config = types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    system_instruction=system_instruction,\n",
    "                    max_output_tokens=max_output_tokens,\n",
    "                    response_modalities=[\"TEXT\"],\n",
    "                    safety_settings=SAFETY_SETTINGS\n",
    "                )\n",
    "            \n",
    "            # We add a different custom configuration if we need it\n",
    "            if new_config:\n",
    "                generate_content_config = new_config\n",
    "            \n",
    "            # For some tasks we need a more specific way to add the contents when prompting the model\n",
    "            # So we need custom parts for it sometimes from the \"types\" objects\n",
    "            if with_parts:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=types.Content(parts=input_prompt),\n",
    "                    config=generate_content_config,\n",
    "                )\n",
    "            # In the simplest form the contents can be expressed as a list [] of simple objects like str and Pillow images\n",
    "            else:\n",
    "                response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=input_prompt,\n",
    "                    config=generate_content_config,\n",
    "                )\n",
    "\n",
    "            if with_tools:\n",
    "                # print(response)\n",
    "                # Include raw response when function calling\n",
    "                completion = response\n",
    "                if with_tokens_info:\n",
    "                    log = {\n",
    "                        \"model\": model_name,\n",
    "                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
    "                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
    "                    }\n",
    "                    return completion, log\n",
    "                return completion\n",
    "            else:\n",
    "                completion = response.text\n",
    "                if with_tokens_info:\n",
    "                    log = {\n",
    "                        \"model\": model_name,\n",
    "                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
    "                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
    "                    }\n",
    "                    # Return the text response and logs (if selected)\n",
    "                    return completion, log\n",
    "                return completion\n",
    "        except Exception as e:\n",
    "             print(f\"Error occurred when generating response, error: {e}\")\n",
    "             return None\n",
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'Predicted label',\n",
    "           ylabel = 'True label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import enum\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "# Define the emotion labels\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "# Define the model to use for few-shot prompting\n",
    "\n",
    "# Schema for the output, the type enum can be used to make a pool of options if what we want is to classify our text selecting only one of them\n",
    "class Emotions(enum.StrEnum):\n",
    "    ANGER = 'anger'\n",
    "    FEAR = 'fear'\n",
    "    JOY = 'joy'\n",
    "    SADNESS = 'sadness'\n",
    "    DISGUST = 'disgust'\n",
    "    SURPRISE = 'surprise'\n",
    "\n",
    "\n",
    "# Function to handle the rate limits of gemini models\n",
    "def handle_rate_limit(request_count, first_request_time, max_calls_per_min):\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Initialize timer on the first request of a new window\n",
    "    if request_count == 0:\n",
    "        first_request_time = current_time\n",
    "\n",
    "    request_count += 1\n",
    "\n",
    "    # If the rate limit is reached\n",
    "\n",
    "    if request_count > max_calls_per_min:\n",
    "        elapsed_time = current_time - first_request_time\n",
    "        if elapsed_time < 60:\n",
    "            wait_time = 60 - elapsed_time\n",
    "            print(f\"Rate limit of {max_calls_per_min} requests per minute reached. Waiting for {wait_time:.2f} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        # Reset for the new window\n",
    "        request_count = 1\n",
    "        first_request_time = time.time()\n",
    "    \n",
    "    return request_count, first_request_time, max_calls_per_min\n",
    "\n",
    "# Function to sample examples per emotion category\n",
    "def sample_few_shots(df, emotions, num_samples=5):\n",
    "    few_shot_examples = {}\n",
    "    for emotion in emotions:\n",
    "        few_shot_examples[emotion] = df[df['emotion'] == emotion].sample(n=num_samples, random_state=42)\n",
    "    return few_shot_examples\n",
    "\n",
    "# Function to build the prompt based on the number of examples (few-shot, 1-shot, zero-shot)\n",
    "def build_prompt(examples, emotions, num_shots=5):\n",
    "    classification_instructions = \"\"\"\n",
    "You will be given a text extracted from social media and your task is to classify the text into one of the following emotion categories: \n",
    "\"anger\" | \"fear\" | \"joy\" | \"sadness\" | \"disgust\" | \"surprise\"\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = classification_instructions + \"\\n\\n\"\n",
    "    \n",
    "    if num_shots > 0:\n",
    "        prompt += f\"Examples: \\n\"\n",
    "        for emotion in emotions:\n",
    "            for _, row in examples[emotion].iterrows():\n",
    "                prompt += f\"Text: {row['text']}\\nClass: {emotion}\\n\\n\" #Show the examples in the same format it will be shown for the classification text\n",
    "                if num_shots == 1:  # If 1-shot, break after the first example for each emotion\n",
    "                    break\n",
    "    return prompt\n",
    "\n",
    "# Function to classify using the LLM with retry for incorrect responses\n",
    "def classify_with_llm(test_text, prompt_base, system_prompt, classes, schema):\n",
    "    response = None\n",
    "    while not response or response not in classes:\n",
    "        full_prompt = f\"{prompt_base}\\nClassification:\\nText: {test_text}\\nClass: \" #The classification text will leave the emotion label to be filled in by the LLM\n",
    "        try:\n",
    "            result = prompt_gemini(input_prompt = [full_prompt], schema = schema, system_instruction = system_prompt)\n",
    "            # print(f\"result: {result} \\n\")\n",
    "            # print(f\"type: {type(result)}\")\n",
    "            if not result:\n",
    "                # In case of giving empty responses with temperature 0.0, we set a higher temperature to seek for different responses\n",
    "                result = prompt_gemini(input_prompt = [full_prompt], schema = schema, system_instruction = system_prompt, temperature=1.0)\n",
    "\n",
    "            try:\n",
    "                # If the result is in the correct format it can be parsed using json\n",
    "                response = json.load(result)\n",
    "            except:\n",
    "                # In case it's not in a json friendly format\n",
    "                # Deleting characters \" and ' in case they appear in our response with the class of the text \n",
    "                response = result.replace('\"', '')    \n",
    "                response = response.replace(\"'\", \"\")  \n",
    "\n",
    "                \n",
    "        # except exceptions.ResourceExhausted as e:\n",
    "        except Exception as e:\n",
    "            print(f\"Waiting to retry... Error: {e}\")\n",
    "            time.sleep(15)\n",
    "            print(f\"test_text: {test_text}\")\n",
    "            return classify_with_llm(test_text, prompt_base, system_prompt, classes, schema) # Retry the request\n",
    "\n",
    "\n",
    "        if response not in classes:  # Retry if not a valid response\n",
    "            print(f\"Invalid response: {response}. Asking for reclassification.\")\n",
    "    return response\n",
    "\n",
    "# Main function to run the experiment with the option for zero-shot, 1-shot, or 5-shot prompting\n",
    "def run_experiment(df_train, df_test, num_test_samples=5, num_shots=5):\n",
    "    # Sample examples for few-shot prompting based on num_shots\n",
    "    if num_shots > 0:\n",
    "        few_shot_examples = sample_few_shots(df_train, emotions, num_samples=num_shots) \n",
    "        prompt_base = build_prompt(few_shot_examples, emotions, num_shots=num_shots)\n",
    "    else:\n",
    "        prompt_base = build_prompt(None, emotions, num_shots=0)  # Zero-shot has no examples\n",
    "\n",
    "    # System prompt for our classification model:\n",
    "    system_prompt = \"You are an emotion classification model for text data. Do not give empty responses, classify according to the list of possible classes.\"\n",
    "\n",
    "    # Prepare to classify the test set\n",
    "    results_data = []\n",
    "\n",
    "    print(prompt_base)\n",
    "    # Sample 20 examples per emotion for the test set to classify\n",
    "    test_samples = sample_few_shots(df_test, emotions, num_samples=num_test_samples)\n",
    "\n",
    "    # Variables to handle rate limit of gemini\n",
    "    request_count = 0\n",
    "    max_calls_per_min = 15 # Gemini 2.5 Flash Lite has this maximum set in the documentation\n",
    "    first_request_time = None\n",
    "\n",
    "    # Classify 20 test examples (5 from each category) and save predictions\n",
    "    for emotion in emotions:\n",
    "        for _, test_row in tqdm(test_samples[emotion].iterrows(), desc=f\"Processing samples for emotion: {emotion}...\", total=num_test_samples):\n",
    "            test_text = test_row['text']\n",
    "            request_count, first_request_time, max_calls_per_min = handle_rate_limit(request_count, first_request_time, max_calls_per_min)  # Check and handle rate limit before each API call\n",
    "            predicted_emotion = classify_with_llm(test_text = test_text, prompt_base = prompt_base, system_prompt = system_prompt, classes = emotions, schema = Emotions)\n",
    "            # Append the results data:\n",
    "            results_data.append({\n",
    "                    'text': test_text,\n",
    "                    'true_emotion': emotion,\n",
    "                    'predicted_emotion': predicted_emotion\n",
    "                })\n",
    "\n",
    "    # Create dataframe to save the results data\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Extract just the true and predicted labels for metrics calculations\n",
    "    true_labels = results_df['true_emotion']\n",
    "    predictions = results_df['predicted_emotion']\n",
    "\n",
    "    output_dir = \"./results/llm_classification_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save the results\n",
    "    filename = f\"{output_dir}/results_samples_{num_test_samples}_shots_{num_shots}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(classification_report(y_true=true_labels, y_pred=predictions))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true=true_labels, y_pred=predictions) \n",
    "    my_tags = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "    plot_confusion_matrix(cm, classes=my_tags, title=f'Confusion matrix for classification with \\n{num_shots}-shot prompting')\n",
    "# If you see '429 RESOURCE_EXHAUSTED' errors it's fine, wait until the data gets processed, it will keep retrying until it finishes\n",
    "\n",
    "# Example of running the experiment with zero-shot prompting\n",
    "run_experiment(train_df1, train_df2 , num_test_samples=20, num_shots=5)\n",
    "#### Pretrained word2vec法( Twitter-100)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_data, y_data = np.array(balanced_df['processed_text']), np.array(balanced_df['emotion'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
    "                                                    test_size = 0.05, random_state = 0)\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load(\"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/Word2Vec-twitter-100\", mmap=\"r\")\n",
    "embedding_dim = w2v.vector_size  # probably 100\n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    # super simple tokenizer – you can swap in your preprocess function if you want\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    \n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # no known words → return zero vector\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "# assuming you have train_df, test_df with processed_text + emotion/sentiment\n",
    "X_train = np.vstack([text_to_vec(t) for t in train_df[\"processed_text\"]])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in test_df[\"processed_text\"]])\n",
    "\n",
    "print(X_train.shape, X_test.shape)  # (n_train, embedding_dim), (n_test, embedding_dim)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "y_train[1:100]\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "# ============================================\n",
    "# 0. Import 套件\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# ============================================\n",
    "# 1. 準備資料：從 balanced_df 拿文字 & 標籤\n",
    "# ============================================\n",
    "# 確保 processed_text 是字串\n",
    "X_data = train_df[\"processed_text\"].astype(str).values\n",
    "y_data = train_df[\"emotion\"].values\n",
    "\n",
    "# 切 train / test（這裡 5% 當 test）\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data  # 依照情緒比例分層抽樣（可選，但通常比較好）\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))\n",
    "\n",
    "# ============================================\n",
    "# 2. 載入預訓練 Word2Vec 模型\n",
    "# ============================================\n",
    "w2v_path = \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/Word2Vec-twitter-100\"\n",
    "w2v = KeyedVectors.load(w2v_path, mmap=\"r\")\n",
    "\n",
    "embedding_dim = w2v.vector_size  # 應該是 100\n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "# ============================================\n",
    "# 3. 定義：文字 -> 平均向量 的函數\n",
    "# ============================================\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    \"\"\"把一則文字轉成平均的 Word2Vec 向量\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    # 簡單 tokenizer（可以換成你前面用的 preprocess）\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    \n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # 如果裡面沒有任何在詞向量中的字，就給 0 向量\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# ============================================\n",
    "# 4. 將 train/test 文字轉成向量 X_train / X_test\n",
    "# ============================================\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_train, embedding_dim)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "\n",
    "# ============================================\n",
    "# 5. 處理標籤：LabelEncoder + 多類分類\n",
    "# ============================================\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# ============================================\n",
    "# 6. 建 MLP 多類分類模型（softmax）\n",
    "# ============================================\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(num_classes, activation=\"softmax\"),  # 多類輸出\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============================================\n",
    "# 7. 訓練模型\n",
    "# ============================================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=30,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. 在 test set 上評估\n",
    "# ============================================\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n",
    "\n",
    "#### Pretrained word2vec法(Google_news 300)\n",
    "# ============================================\n",
    "# 0. Import 套件\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "# ============================================\n",
    "# 1. 準備資料：從 balanced_df 拿文字 & 標籤\n",
    "# ============================================\n",
    "# 確保 processed_text 是字串\n",
    "X_data = transformed_df[\"processed_text\"].astype(str).values\n",
    "y_data = transformed_df[\"emotion\"].values\n",
    "\n",
    "# 切 train / test（這裡 5% 當 test）\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data  # 依照情緒比例分層抽樣（可選，但通常比較好）\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))\n",
    "\n",
    "# ============================================\n",
    "# 2. 載入預訓練 Word2Vec 模型\n",
    "# ============================================\n",
    "w2v_path = \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "embedding_dim = w2v.vector_size  \n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "# ============================================\n",
    "# 3. 定義：文字 -> 平均向量 的函數\n",
    "# ============================================\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    \"\"\"把一則文字轉成平均的 Word2Vec 向量\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    # 簡單 tokenizer（可以換成你前面用的 preprocess）\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    \n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # 如果裡面沒有任何在詞向量中的字，就給 0 向量\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# ============================================\n",
    "# 4. 將 train/test 文字轉成向量 X_train / X_test\n",
    "# ============================================\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_train, embedding_dim)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "\n",
    "# ============================================\n",
    "# 5. 處理標籤：LabelEncoder + 多類分類\n",
    "# ============================================\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# ============================================\n",
    "# 6. 建 MLP 多類分類模型（softmax）\n",
    "# ============================================\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(num_classes, activation=\"softmax\"),  # 多類輸出\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============================================\n",
    "# 7. 訓練模型\n",
    "# ============================================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=30,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. 在 test set 上評估\n",
    "# ============================================\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 0. Import 套件\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ============================================\n",
    "# 1. 準備資料：從 train_df 拿文字 & 標籤\n",
    "# ============================================\n",
    "X_data = balanced_df[\"text\"].astype(str).values\n",
    "y_data = balanced_df[\"emotion\"].values\n",
    "\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))\n",
    "\n",
    "# ============================================\n",
    "# 2. 載入 GoogleNews 300d Word2Vec\n",
    "# ============================================\n",
    "w2v_path = \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/GoogleNews-vectors-negative300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "embedding_dim = w2v.vector_size  # 300\n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "# ============================================\n",
    "# 3. 建一個 TF-IDF，之後做「加權平均」用\n",
    "#    注意：這裡 token pattern 不 lower，讓大小寫保留\n",
    "# ============================================\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"\\b\\w+\\b\")  # 很單純的 word tokenizer\n",
    "tfidf.fit(X_train_text)  # 只在 train 上 fit\n",
    "\n",
    "idf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "# ============================================\n",
    "# 4. 定義：文字 -> TF-IDF 加權 Word2Vec 向量\n",
    "# ============================================\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim, idf=idf_dict):\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    # 不做 lower，盡量跟 GoogleNews 的詞一致\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "    vecs = []\n",
    "    weights = []\n",
    "\n",
    "    for w in tokens:\n",
    "        if w in model.key_to_index:\n",
    "            vecs.append(model[w])\n",
    "            # 用 tf-idf 的 idf 當權重，找不到就給 1.0\n",
    "            weights.append(idf.get(w.lower(), 1.0))\n",
    "\n",
    "    if not vecs:\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    vecs = np.vstack(vecs)\n",
    "    weights = np.array(weights).reshape(-1, 1)\n",
    "\n",
    "    # 加權平均\n",
    "    weighted = (vecs * weights).sum(axis=0) / weights.sum()\n",
    "    return weighted.astype(\"float32\")\n",
    "\n",
    "# ============================================\n",
    "# 5. 將 train/test 文字轉成向量，並做標準化\n",
    "# ============================================\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# ============================================\n",
    "# 6. 處理標籤：LabelEncoder + 多類分類\n",
    "# ============================================\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# ============================================\n",
    "# 7. 建 MLP 多類分類模型（加 Dropout & callbacks）\n",
    "# ============================================\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "rlrop = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. 訓練模型\n",
    "# ============================================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, rlrop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 9. 在 test set 上評估\n",
    "# ============================================\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 先對 X_test 做預測（而不是 Kaggle test_df）\n",
    "proba_test = model.predict(X_test)\n",
    "pred_test_idx = np.argmax(proba_test, axis=1)\n",
    "true_test_idx = label_encoder.transform(y_test_raw)\n",
    "\n",
    "cm = confusion_matrix(true_test_idx, pred_test_idx)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "test_df\n",
    "predict = np.vstack([text_to_vec(t) for t in test_df['text']])\n",
    "pred_result= model.predict(predict)\n",
    "pred_result\n",
    "pred_result = model.predict(predict)\n",
    "pred_class_idx = np.argmax(pred_result, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(pred_class_idx)\n",
    "result_df = pd.DataFrame({\"id\" : test_df[\"post_id\"], \"emotion\" :pred_labels})\n",
    "result_df.emotion.value_counts()\n",
    "result_df = result_df.set_index(\"id\")\n",
    "\n",
    "result_df.to_csv(\"submission5.csv\") \n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "test_df\n",
    "### Word2vec Medium\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv_path = \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "wv = KeyedVectors.load_word2vec_format(wv_path, binary=True)\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    # print(wv_res)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "# Creating our tokenizer function\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "\n",
    "\n",
    "    # print(doc)\n",
    "    # print(type(doc))\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "\n",
    "    # print(mytokens)\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "tokened_df = pd.DataFrame()\n",
    "tokened_df['tokens'] = train_df['processed_text'].apply(spacy_tokenizer)\n",
    "tokened_df.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "tokened_df['emotion'] = label_encoder.fit_transform(train_df[\"emotion\"])\n",
    "tokened_df['vec'] = tokened_df['tokens'].apply(sent_vec)\n",
    "tokened_df\n",
    "X = tokened_df['vec'].to_list()\n",
    "y = tokened_df['emotion'].to_list()\n",
    "X[0]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)\n",
    "from sklearn import metrics\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "print(\"Logistic Regression Precision (macro):\",\n",
    "      metrics.precision_score(y_test, predicted, average='macro'))\n",
    "\n",
    "print(\"Logistic Regression Recall (macro):\",\n",
    "      metrics.recall_score(y_test, predicted, average='macro'))\n",
    "\n",
    "print(\"Logistic Regression F1 (macro):\",\n",
    "      metrics.f1_score(y_test, predicted, average='macro'))\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 如果是 GoogleNews-vectors-negative300.bin\n",
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\",\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "EMB_DIM = w2v.vector_size  # 通常是 300\n",
    "\n",
    "def tokens_to_vec(tokens):\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        if tok in w2v.key_to_index:  # 新版 gensim 的屬性\n",
    "            vecs.append(w2v[tok])\n",
    "    if len(vecs) == 0:\n",
    "        # 沒有任何 token 在 vocab 裡 → 回傳 0 向量或小亂數\n",
    "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "# df.columns = ['tokens', 'emotion']\n",
    "X = np.vstack(tokened_df['tokens'].apply(tokens_to_vec).values)\n",
    "y = tokened_df['emotion'].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 如果是 GoogleNews-vectors-negative300.bin\n",
    "w2v_path = \"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "embedding_dim = w2v.vector_size  \n",
    "\n",
    "def tokens_to_vec(tokens):\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        tok2 = tok\n",
    "        if tok2 not in w2v.key_to_index:\n",
    "            tok2 = tok.lower()\n",
    "        if tok2 in w2v.key_to_index:\n",
    "            vecs.append(w2v[tok2])\n",
    "    if not vecs:\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "# df.columns = ['tokens', 'emotion']\n",
    "X = np.vstack(tokened_df['tokens'].apply(tokens_to_vec).values)\n",
    "y = tokened_df['emotion'].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. 訓練模型\n",
    "# ============================================\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. 在 test set 上評估\n",
    "# ============================================\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "predict_token = pd.DataFrame()\n",
    "predict_token['tokens']  = test_df['processed_text'].apply(spacy_tokenizer)\n",
    "predict_token ['vec'] = predict_token['tokens'].apply(sent_vec)\n",
    "predict_token \n",
    "X_pred = np.vstack(predict_token['vec'].to_numpy())   # shape: (n_samples, 300)\n",
    "\n",
    "# 2. run the model\n",
    "probs = model.predict(X_pred)                         # shape: (n_samples, 6)\n",
    "\n",
    "# 3. get predicted class index\n",
    "y_pred_idx = probs.argmax(axis=1)\n",
    "\n",
    "# 4. if you used LabelEncoder before:\n",
    "#    le = LabelEncoder(); y = le.fit_transform(...)\n",
    "#    then you can map back to labels:\n",
    "#    y_pred = le.inverse_transform(y_pred_idx)\n",
    "\n",
    "predict_token[\"pred_label_idx\"] = y_pred_idx\n",
    "predict_token.pred_label_idx.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 先對 X_test 做預測（而不是 Kaggle test_df）\n",
    "proba_test = model.predict(X_test)\n",
    "pred_test_idx = np.argmax(proba_test, axis=1)\n",
    "true_test_idx = y_test\n",
    "\n",
    "cm = confusion_matrix(true_test_idx, pred_test_idx)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "### Roberta\n",
    "! pip install transormers\n",
    "! pip install torch\n",
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set Up Your HuggingFace API Token\n",
    "HUGGINGFACE_API_TOKEN = 'API token'\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACE_API_TOKEN\n",
    "\n",
    "# Loading a Pre-Trained Model from HuggingFace Hub\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Creating a Function to Run the Application\n",
    "def run_classification(text):\n",
    "    result = classifier(text)\n",
    "    return result\n",
    "\n",
    "# Running the Application\n",
    "input_text = \"I love using HuggingFace models for NLP tasks!\"\n",
    "result = run_classification(input_text)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Classification: {result}\")\n",
    "\n",
    "#### Kaggle TFIDF LogRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_df['processed_text']\n",
    "y = train_df['emotion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Predict and check accuracy\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "test_tfidf = tfidf.transform(test_df['text'])\n",
    "y_pred = model.predict(test_tfidf)\n",
    "\n",
    "result_df = pd.DataFrame({\"id\" : test_df[\"post_id\"], \"emotion\" :y_pred})\n",
    "result_df\n",
    "result_df = result_df.set_index([\"id\"])\n",
    "result_df.to_csv(\"submission_TFIDF.csv\") \n",
    "#### LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['emotion_encoded'] = label_encoder.fit_transform(train_df['emotion'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df['processed_text'], train_df['emotion_encoded'], \n",
    "    test_size=0.1, random_state=42, stratify=train_df['emotion_encoded']\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "max_words = 20000  # vocab size\n",
    "max_len = 50       # max sequence length (adjust based on EDA)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Convert to numpy\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "model_ffnn = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_ffnn.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_ffnn = model_ffnn.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=256\n",
    ")\n",
    "from tensorflow.keras.layers import LSTM, SpatialDropout1D\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    SpatialDropout1D(0.3),\n",
    "    LSTM(128, dropout=0.3, recurrent_dropout=0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=256\n",
    ")\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "\n",
    "model_bilstm = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    Bidirectional(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bilstm.compile(loss='sparse_categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "history_bilstm = model_bilstm.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=256\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(model_bilstm.predict(X_test_pad), axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - BiLSTM\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "test_df\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "def text_to_vec(text):\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    vecs = [w2v[w] for w in tokens if w in w2v.key_to_index]\n",
    "    if not vecs:\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "X_train = np.vstack([text_to_vec(t) for t in train_df[\"processed_text\"]])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in test_df[\"processed_text\"]])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(embedding_dim,)),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = pd.read_pickle(\"/Users/daidisheng/Desktop/研究所/DM2025Labs/DM2025-Lab2-Exercise/dm-lab-2-private-competition/Tokenizer.pickle\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_length = min(len(word_index) + 1, w2v_model.vector_size * 1000000)  # or just len(word_index)+1\n",
    "\n",
    "Embedding_dimensions = w2v_model.vector_size  # should be 100 for twitter-100\n",
    "\n",
    "#embedding_matrix = np.zeros((len(word_index) + 1, Embedding_dimensions), dtype=\"float32\")\n",
    "\n",
    "#for word, idx in word_index.items():\n",
    "   # if idx >= embedding_matrix.shape[0]:\n",
    "      #  continue\n",
    "    #if word in w2v_model.key_to_index:   # gensim >= 4\n",
    "      #  embedding_matrix[idx] = w2v_model[word]\n",
    "    #else:\n",
    "        # OOV word: keep zeros or random small noise\n",
    "        # embedding_matrix[idx] = np.random.normal(scale=0.6, size=(Embedding_dimensions,))\n",
    "       # pass\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n",
    "\n",
    "\n",
    "for word, token in tokenizer.word_index.items():\n",
    "    if w2v_model.__contains__(word):\n",
    "        embedding_matrix[token] = w2v_model.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(Word2vec_train_data,\n",
    "                 vector_size=Embedding_dimensions,\n",
    "                 workers=8,\n",
    "                 min_count=5)\n",
    "\n",
    "print(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n",
    "X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape :\", X_test.shape)\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n",
    "\n",
    "def getModel():\n",
    "    embedding_layer = Embedding(input_dim = vocab_length,\n",
    "                                output_dim = Embedding_dimensions,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=input_length,\n",
    "                                trainable=True)\n",
    "\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n",
    "        Conv1D(100, 5, activation='relu'),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ],\n",
    "    name=\"Sentiment_Model\")\n",
    "    return model\n",
    "\n",
    "training_model = getModel()\n",
    "training_model.summary()\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n",
    "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = training_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "train_df1 = balanced_df.sample(frac=0.95, random_state=42)\n",
    "train_df2  = balanced_df.drop(train_df1.index)\n",
    "import keras\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "\n",
    "BOW_500.fit(df['processed_text'])\n",
    "\n",
    "X_train = BOW_500.transform(train_df1['processed_text'])\n",
    "y_train = train_df1['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(train_df2['processed_text'])\n",
    "y_test = train_df2['emotion']\n",
    "\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train) # 用X預測Y\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pr\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "# deal with label (string -> one-hot)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:10])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "def label_encode(le, labels): \n",
    "    enc = le.transform(labels) # categorize成 [1,2,3 4]\n",
    "    return keras.utils.to_categorical(enc)  # (方便神經元)把catetgory 變成 onehotencoding -> 二位元  [1, 0, 0, 0] ~ [0, 0, 0, 1]\n",
    "\n",
    "def label_decode(le, one_hot_label):  \n",
    "    dec = np.argmax(one_hot_label, axis=1) # 找出每一行中哪個位置是 1（或最大值）→ 轉回整數編碼\n",
    "    return le.inverse_transform(dec) # 把整數標籤轉回原始文字\n",
    "\n",
    "\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "# I/O check\n",
    "input_shape = X_train.shape[1] # 500筆資料\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_) # 情緒標籤\n",
    "print('output_shape: ', output_shape)\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # input 500個 Features\n",
    "X = model_input\n",
    "\n",
    "X_W1 = Dense(units=64)(X)  # 將500個Features濃縮成64個Features\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 再做一次以獲得更細緻的情緒Feature\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 將每個64Feature都歸類成一種情緒（一共6種），並回推一開始500格分別代表哪一種情緒\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam', # adam adjusts the weights automatically to minimize errors.\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy is used for multi-class classification.\n",
    "              metrics=['accuracy']) # accuracy tells you how many predictions are correct.\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "# training setting\n",
    "epochs = 50 # Epoch = 整個訓練資料要「重複學習」25輪\n",
    "batch_size = 64 # 每次拿 32 筆資料來更新權重\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')\n",
    "acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\n",
    "loss, val_loss = history.history['loss'], history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r',\n",
    " label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))\n",
    "## 3. Model Implementation Steps\n",
    "### Add the code related to the model implementation steps in cells inside this section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python\n(dm2025lab2)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

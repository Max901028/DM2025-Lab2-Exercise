{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6476b73",
   "metadata": {},
   "source": [
    "# Using machine learning for sentiment analysis\n",
    "The goal of this project is to train a Model for Text Sentiment Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0dc88",
   "metadata": {},
   "source": [
    "## Data Preprocess & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae15a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_type': 'post', '_source': {'post': {'post_id': '0x61fc95', 'text': 'We got the ranch, loaded our guns and sat up till sunrise.', 'hashtags': []}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_json('dm-lab-2-private-competition/final_posts.json')\n",
    "\n",
    "print(df_raw.iloc[0,0])\n",
    "df = pd.DataFrame()\n",
    "df[\"post_id\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"post_id\"])\n",
    "df[\"text\"]    = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"text\"])\n",
    "df[\"hashtags\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844aed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  emotion  ident  \n",
       "0                                     []      NaN   test  \n",
       "1                                     []      joy  train  \n",
       "2                                     []     fear  train  \n",
       "3                                     []      joy  train  \n",
       "4                                     []      NaN   test  \n",
       "...                                  ...      ...    ...  \n",
       "64166                                 []    anger  train  \n",
       "64167                                 []      joy  train  \n",
       "64168                                 []      NaN   test  \n",
       "64169  [texans, astros, sadness, losers]  sadness  train  \n",
       "64170                                 []      NaN   test  \n",
       "\n",
       "[64171 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ident = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "emotion['post_id'] = emotion['id']\n",
    "df = df.merge(emotion[['post_id', 'emotion']], on='post_id', how='left')\n",
    "df['ident'] = ident['split']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7b977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>Thank you so much❤️</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>Stinks because ive been in this program for a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64164</th>\n",
       "      <td>0xd740f2</td>\n",
       "      <td>why is everybody seem sp serious?</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64165</th>\n",
       "      <td>0x99267e</td>\n",
       "      <td>You can cross fuck off, its 10f all winter in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47890 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text  \\\n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "7      0x2ffb63                                Thank you so much❤️   \n",
       "9      0x989146  Stinks because ive been in this program for a ...   \n",
       "...         ...                                                ...   \n",
       "64164  0xd740f2                  why is everybody seem sp serious?   \n",
       "64165  0x99267e  You can cross fuck off, its 10f all winter in ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "\n",
       "                                hashtags  emotion  ident  \n",
       "1                                     []      joy  train  \n",
       "2                                     []     fear  train  \n",
       "3                                     []      joy  train  \n",
       "7                                     []      joy  train  \n",
       "9                                     []      joy  train  \n",
       "...                                  ...      ...    ...  \n",
       "64164                                 []      joy  train  \n",
       "64165                                 []    anger  train  \n",
       "64166                                 []    anger  train  \n",
       "64167                                 []      joy  train  \n",
       "64169  [texans, astros, sadness, losers]  sadness  train  \n",
       "\n",
       "[47890 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['ident'] == 'train']\n",
    "test_df =  df[df['ident'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff936f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>Thank you so much❤️</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>Stinks because ive been in this program for a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id                                               text hashtags  \\\n",
       "1  0x35663e  I bet there is an army of married couples who ...       []   \n",
       "2  0xc78afe                         This could only end badly.       []   \n",
       "3  0x90089c  My sister squeezed a lime in her milk when she...       []   \n",
       "7  0x2ffb63                                Thank you so much❤️       []   \n",
       "9  0x989146  Stinks because ive been in this program for a ...       []   \n",
       "\n",
       "  emotion  ident  \n",
       "1     joy  train  \n",
       "2    fear  train  \n",
       "3     joy  train  \n",
       "7     joy  train  \n",
       "9     joy  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c367dd",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Text Preprocessing is traditionally an important step for Natural Language Processing (NLP) tasks. \n",
    "\n",
    "It transforms text into a more digestible form so that deep learning algorithms can perform better.\n",
    "\n",
    "The Preprocessing steps taken are:\n",
    "\n",
    "1. Lower Casing: Each text is converted to lowercase.\n",
    "\n",
    "2. Replacing URLs: Links starting with 'http' or 'https' or 'www' are replaced by '<url>'.\n",
    "\n",
    "3. Replacing Usernames: Replace @Usernames with word '<user>'. [eg: '@Kaggle' to '<user>'].\n",
    "\n",
    "4. Replacing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n",
    "\n",
    "5. Replacing Emojis: Replace emojis by using a regex expression. [eg: ':)' to '<smile>']\n",
    "\n",
    "6. Replacing Contractions: Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n",
    "\n",
    "7. Removing Non-Alphabets: Replacing characters except Digits, Alphabets and pre-defined Symbols with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d57ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/2270860636.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['processed_text'] = train_df.text.apply(preprocess_apply)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "contractions = pd.read_csv('dm-lab-2-private-competition/contractions.csv.xls', index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# Defining regex for emojis\n",
    "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
    "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
    "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    # Replace all emojis.\n",
    "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
    "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
    "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
    "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
    "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
    "\n",
    "    for contraction, replacement in contractions_dict.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "\n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
    "\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "train_df['processed_text'] = train_df.text.apply(preprocess_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cc3595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I bet there is an army of married couples who did the same exact thing.\n",
      "processed_text: joy \n",
      "\n",
      "Text: This could only end badly.\n",
      "processed_text: fear \n",
      "\n",
      "Text: My sister squeezed a lime in her milk when she was 12. Same thing happened, but we told her it would happen AFTER she did it ..\n",
      "processed_text: joy \n",
      "\n",
      "Text: Thank you so much❤️\n",
      "processed_text: joy \n",
      "\n",
      "Text: Stinks because ive been in this program for a year with no pay.....back to the drawing board.\n",
      "processed_text: joy \n",
      "\n",
      "Text: The overall response is try and empower women, abolish prostitution and stop giving lazy men money because they want to live out their idiotic fantasy lives. \n",
      "processed_text: anger \n",
      "\n",
      "Text: Your market sucks\n",
      "processed_text: anger \n",
      "\n",
      "Text: here’s hoping the same is true for me!\n",
      "processed_text: joy \n",
      "\n",
      "Text: She looks like a televangelist.\n",
      "processed_text: joy \n",
      "\n",
      "Text: Rap that will Cut other raper's throat. Who said that? @Paedeezy #badd #wicked. #bright city lights\n",
      "processed_text: anger \n",
      "\n",
      "Text: She’s a good person who stands up for people not like her, and they can’t stand that.\n",
      "processed_text: joy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the texts now turn into this\n",
    "count=0\n",
    "for row in train_df.itertuples():\n",
    "    print(\"Text:\", row[2])\n",
    "    print('processed_text:', row[4],\"\\n\")\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26194f46",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e9a707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45495\n",
      "Test size : 2395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "\n",
    "\n",
    "X_data = train_df[\"processed_text\"].astype(str).values\n",
    "y_data = train_df[\"emotion\"].values\n",
    "\n",
    "# 切 train / test（這裡 5% 當 test）\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data  # 依照情緒比例分層抽樣\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dd0638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 300\n",
      "X_train shape: (45495, 300)\n",
      "X_test shape : (2395, 300)\n"
     ]
    }
   ],
   "source": [
    "# 載入 Pretrained Google_news Word2Vec 模型\n",
    "\n",
    "w2v_path = \"dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "embedding_dim = w2v.vector_size  \n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    \"\"\"把一則文字轉成平均的 Word2Vec 向量\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # 如果裡面沒有任何在詞向量中的字，就給 0 向量\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "#  將文字轉成向量 \n",
    "\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_train, embedding_dim)\n",
    "print(\"X_test shape :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "num_classes: 6\n"
     ]
    }
   ],
   "source": [
    "# Python can't understand emotions like \"sadness\" or \"joy\", so here we have to change all 6 emotions into a numeric category of 0~6\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc9e9bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m77,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\"),  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "rlr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0e09e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - accuracy: 0.6039 - loss: 1.0566 - val_accuracy: 0.5888 - val_loss: 1.0786 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6100 - loss: 1.0369 - val_accuracy: 0.5895 - val_loss: 1.0857 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754us/step - accuracy: 0.6174 - loss: 1.0176 - val_accuracy: 0.5943 - val_loss: 1.0738 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.6253 - loss: 0.9958 - val_accuracy: 0.5949 - val_loss: 1.0734 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.6328 - loss: 0.9709 - val_accuracy: 0.5921 - val_loss: 1.0796 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1223/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.6446 - loss: 0.9348\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.6434 - loss: 0.9470 - val_accuracy: 0.5947 - val_loss: 1.0848 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - accuracy: 0.6618 - loss: 0.8963 - val_accuracy: 0.5967 - val_loss: 1.1068 - learning_rate: 5.0000e-04\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - accuracy: 0.6117 - loss: 1.0273\n",
      "Test loss: 1.0273  |  Test acc: 0.6117\n"
     ]
    }
   ],
   "source": [
    "# Input training data\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=[es, rlr],\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137cc69",
   "metadata": {},
   "source": [
    "##  Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d05d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/3121004903.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we got the ranch  loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>and that got my head bobbing a little bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>same  glad it is not just out store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>like always i will wait and see thanks for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>thereis a bit of room between  not loving sub ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we all do it sometimes do not worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>this new year i visited more relatives than us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>r u a dad or did ur dad leave u both have bad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>pre prepare direction plays hale and hearty si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text hashtags  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...       []   \n",
       "4      0xaba820         and that got my head bobbing a little bit.       []   \n",
       "5      0x66e44d                Same. Glad it's not just out store.       []   \n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...       []   \n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...       []   \n",
       "...         ...                                                ...      ...   \n",
       "64146  0x0f273c                We all do it sometimes don't worry.       []   \n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...       []   \n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...       []   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...       []   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...       []   \n",
       "\n",
       "      emotion ident                                     processed_text  \n",
       "0         NaN  test  we got the ranch  loaded our guns and sat up t...  \n",
       "4         NaN  test         and that got my head bobbing a little bit   \n",
       "5         NaN  test               same  glad it is not just out store   \n",
       "6         NaN  test  like always i will wait and see thanks for the...  \n",
       "8         NaN  test  thereis a bit of room between  not loving sub ...  \n",
       "...       ...   ...                                                ...  \n",
       "64146     NaN  test               we all do it sometimes do not worry   \n",
       "64150     NaN  test  this new year i visited more relatives than us...  \n",
       "64157     NaN  test  r u a dad or did ur dad leave u both have bad ...  \n",
       "64168     NaN  test  i got my first raspberry from a crowd surfer f...  \n",
       "64170     NaN  test  pre prepare direction plays hale and hearty si...  \n",
       "\n",
       "[16281 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unclassified text data\n",
    "test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6350f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.22979933e-01, 3.68012721e-03, 2.64608636e-02, 5.12456417e-01,\n",
       "        7.52074877e-03, 2.26901829e-01],\n",
       "       [5.79092763e-02, 5.67220151e-03, 6.81349695e-01, 1.44356295e-01,\n",
       "        3.10226120e-02, 7.96900317e-02],\n",
       "       [9.01271924e-02, 8.57952144e-03, 7.43259070e-03, 6.33115232e-01,\n",
       "        1.22691981e-01, 1.38053477e-01],\n",
       "       ...,\n",
       "       [2.22188517e-01, 3.17916125e-02, 8.08604062e-03, 4.45347011e-01,\n",
       "        2.19614059e-01, 7.29727298e-02],\n",
       "       [3.13539892e-01, 1.63444970e-02, 3.00939441e-01, 3.13508034e-01,\n",
       "        4.22553048e-02, 1.34128630e-02],\n",
       "       [1.01556545e-02, 1.40085549e-05, 1.01512715e-05, 9.83254015e-01,\n",
       "        9.00041414e-05, 6.47614058e-03]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.vstack([text_to_vec(t) for t in test_df['processed_text']])\n",
    "pred_result= model.predict(predict)\n",
    "pred_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0705fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16281, 6)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row has 6 numbers representing possible emotions. \n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "pred_result.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e50a7f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text_id                                               text emotion\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...     joy\n",
       "4      0xaba820         and that got my head bobbing a little bit.    fear\n",
       "5      0x66e44d                Same. Glad it's not just out store.     joy\n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...     joy\n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...     joy\n",
       "...         ...                                                ...     ...\n",
       "64146  0x0f273c                We all do it sometimes don't worry.     joy\n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...   anger\n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...     joy\n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   anger\n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...     joy\n",
       "\n",
       "[16281 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class_idx = np.argmax(pred_result, axis=1) # For each text choose the most likely emotion\n",
    "pred_labels = label_encoder.inverse_transform(pred_class_idx) # Changing the numeric emotion category back to words.\n",
    "result_df = pd.DataFrame({\"Text_id\" : test_df[\"post_id\"],\"text\": test_df[\"text\"], \"emotion\" :pred_labels})\n",
    "result_df\n",
    "# From the train we learnt that this model has a 62% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python\n(dm2025lab2)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 戴迪生\n",
    "\n",
    "Student ID: 113351037\n",
    "\n",
    "GitHub ID: MaxTai1028\n",
    "\n",
    "Kaggle name:MaxTai1028\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "\n",
    "![DM Lab 2 Leaderboard](Screenshot_2025-12-02_at_13.46.34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "\n",
    "\n",
    "I turned the provided data into a single DataFrame. Then I split the data into training and split the data into training and impementation testing.\n",
    "\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "It transforms text into a more digestible form so that deep learning algorithms can perform better.\n",
    "\n",
    "The Preprocessing steps taken are:\n",
    "\n",
    "1. Lower Casing: Each text is converted to lowercase.\n",
    "\n",
    "2. Replacing URLs: Links starting with 'http' or 'https' or 'www' are replaced by '<url>'.\n",
    "\n",
    "3. Replacing Usernames: Replace @Usernames with word '<user>'. [eg: '@Kaggle' to '<user>'].\n",
    "\n",
    "4. Replacing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n",
    "\n",
    "5. Replacing Emojis: Replace emojis by using a regex expression. [eg: ':)' to '<smile>']\n",
    "\n",
    "6. Replacing Contractions: Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n",
    "\n",
    "7. Removing Non-Alphabets: Replacing characters except Digits, Alphabets and pre-defined Symbols with a space.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "1. The dataset is further separated into training and testing subsets from the original trainning data, with five percent reserved for testing. \n",
    "\n",
    "2. After the data is partitioned, I loaded the pretrained Google News Word2Vec model, which provides 300-dimensional word embeddings derived from large-scale English corpora. To integrate these embeddings into the model, a function is defined to convert each text sample into a single vector. This function tokenizes the input sentence, identifies the words that exist in the Word2Vec vocabulary, retrieves their corresponding vectors, and then averages them to generate a fixed-length representation of the entire sentence. If no valid tokens are identified, a zero vector is used to maintain dimensional consistency.\n",
    "\n",
    "3. Then all training and testing text entries are transformed into numerical embeddings. Each sample becomes a dense vector that can be directly fed into a machine-learning model. Since neural networks cannot directly interpret string labels such as “sadness” or “joy,”  the emotion categories are encoded into numerical form using a label encoder. This ensures a consistent mapping across both training and evaluation stages.\n",
    "\n",
    "4. The model is then constructed using a sequential neural-network architecture. The network includes 2 dense layers with ReLU activation functions, a dropout layer of 0.5 designed to mitigate overfitting, and a softmax output layer that produces emotion probabilities for the text. The model is compiled with the Adam optimizer and trained using sparse categorical cross-entropy.\n",
    "\n",
    "5. To support stability during training, the script incorporates early stopping and adaptive learning-rate scheduling. Early stopping monitors the validation loss and halts training when improvements stop occurring, while the learning-rate scheduler reduces the learning rate when progress plateaus. These techniques enhance the efficiency of the training process and reduce the likelihood of overfitting.\n",
    "\n",
    "6. The model is then trained on the vectorized dataset, with a portion of the training data allocated for validation. Through iterative optimization, the network learns to associate the Word2Vec-derived sentence embeddings with their respective emotion categories. Finally, the model is evaluated on the previously isolated test set, yielding a test loss and test accuracy that reflect its generalization capability on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "I tried using sample(frac) to reduce the entries for \"joy\" in the training data but failed. The model's accuracy and loss didn't improve. In fact, it got even worse.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "A friend of mine who also join the private competition said that only by doing some feature engineering like PCA, Normalize, or UMAP to reduce the data dimention will the model accuracy improve. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Development (10 pts Required)\n",
    "### Using machine learning and pretrained Word2vec Google_news_300 for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preprocess & Cleansing\n",
    "I turned the provided data into a single DataFrame. Then I split the data into training and split the data into training and impementation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_type': 'post', '_source': {'post': {'post_id': '0x61fc95', 'text': 'We got the ranch, loaded our guns and sat up till sunrise.', 'hashtags': []}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_json('dm-lab-2-private-competition/final_posts.json')\n",
    "\n",
    "print(df_raw.iloc[0,0])\n",
    "df = pd.DataFrame()\n",
    "df[\"post_id\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"post_id\"])\n",
    "df[\"text\"]    = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"text\"])\n",
    "df[\"hashtags\"] = df_raw[\"root\"].apply(lambda x: x[\"_source\"][\"post\"][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>anger</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  emotion  ident  \n",
       "0                                     []      NaN   test  \n",
       "1                                     []      joy  train  \n",
       "2                                     []     fear  train  \n",
       "3                                     []      joy  train  \n",
       "4                                     []      NaN   test  \n",
       "...                                  ...      ...    ...  \n",
       "64166                                 []    anger  train  \n",
       "64167                                 []      joy  train  \n",
       "64168                                 []      NaN   test  \n",
       "64169  [texans, astros, sadness, losers]  sadness  train  \n",
       "64170                                 []      NaN   test  \n",
       "\n",
       "[64171 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ident = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "emotion = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "emotion['post_id'] = emotion['id']\n",
    "df = df.merge(emotion[['post_id', 'emotion']], on='post_id', how='left')\n",
    "df['ident'] = ident['split']\n",
    "df # turn into a consider DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['ident'] == 'train']\n",
    "test_df =  df[df['ident'] == 'test']\n",
    "# Splitting the data into training and impementation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>Thank you so much❤️</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>Stinks because ive been in this program for a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id                                               text hashtags  \\\n",
       "1  0x35663e  I bet there is an army of married couples who ...       []   \n",
       "2  0xc78afe                         This could only end badly.       []   \n",
       "3  0x90089c  My sister squeezed a lime in her milk when she...       []   \n",
       "7  0x2ffb63                                Thank you so much❤️       []   \n",
       "9  0x989146  Stinks because ive been in this program for a ...       []   \n",
       "\n",
       "  emotion  ident  \n",
       "1     joy  train  \n",
       "2    fear  train  \n",
       "3     joy  train  \n",
       "7     joy  train  \n",
       "9     joy  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "It transforms text into a more digestible form so that deep learning algorithms can perform better.\n",
    "\n",
    "The Preprocessing steps taken are:\n",
    "\n",
    "1. Lower Casing: Each text is converted to lowercase.\n",
    "\n",
    "2. Replacing URLs: Links starting with 'http' or 'https' or 'www' are replaced by '<url>'.\n",
    "\n",
    "3. Replacing Usernames: Replace @Usernames with word '<user>'. [eg: '@Kaggle' to '<user>'].\n",
    "\n",
    "4. Replacing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. [eg: 'Heyyyy' to 'Heyy']\n",
    "\n",
    "5. Replacing Emojis: Replace emojis by using a regex expression. [eg: ':)' to '<smile>']\n",
    "\n",
    "6. Replacing Contractions: Replacing contractions with their meanings. [eg: \"can't\" to 'can not']\n",
    "\n",
    "7. Removing Non-Alphabets: Replacing characters except Digits, Alphabets and pre-defined Symbols with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/2270860636.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['processed_text'] = train_df.text.apply(preprocess_apply)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "contractions = pd.read_csv('dm-lab-2-private-competition/contractions.csv.xls', index_col='Contraction')\n",
    "contractions.index = contractions.index.str.lower()\n",
    "contractions.Meaning = contractions.Meaning.str.lower()\n",
    "contractions_dict = contractions.to_dict()['Meaning']\n",
    "\n",
    "# Defining regex patterns.\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "# Defining regex for emojis\n",
    "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
    "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
    "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    # Replace all emojis.\n",
    "    tweet = re.sub(r'<3', '<heart>', tweet)\n",
    "    tweet = re.sub(smileemoji, '<smile>', tweet)\n",
    "    tweet = re.sub(sademoji, '<sadface>', tweet)\n",
    "    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n",
    "    tweet = re.sub(lolemoji, '<lolface>', tweet)\n",
    "\n",
    "    for contraction, replacement in contractions_dict.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "\n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = re.sub(alphaPattern, ' ', tweet)\n",
    "\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "train_df['processed_text'] = train_df.text.apply(preprocess_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I bet there is an army of married couples who did the same exact thing.\n",
      "processed_text: joy \n",
      "\n",
      "Text: This could only end badly.\n",
      "processed_text: fear \n",
      "\n",
      "Text: My sister squeezed a lime in her milk when she was 12. Same thing happened, but we told her it would happen AFTER she did it ..\n",
      "processed_text: joy \n",
      "\n",
      "Text: Thank you so much❤️\n",
      "processed_text: joy \n",
      "\n",
      "Text: Stinks because ive been in this program for a year with no pay.....back to the drawing board.\n",
      "processed_text: joy \n",
      "\n",
      "Text: The overall response is try and empower women, abolish prostitution and stop giving lazy men money because they want to live out their idiotic fantasy lives. \n",
      "processed_text: anger \n",
      "\n",
      "Text: Your market sucks\n",
      "processed_text: anger \n",
      "\n",
      "Text: here’s hoping the same is true for me!\n",
      "processed_text: joy \n",
      "\n",
      "Text: She looks like a televangelist.\n",
      "processed_text: joy \n",
      "\n",
      "Text: Rap that will Cut other raper's throat. Who said that? @Paedeezy #badd #wicked. #bright city lights\n",
      "processed_text: anger \n",
      "\n",
      "Text: She’s a good person who stands up for people not like her, and they can’t stand that.\n",
      "processed_text: joy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the texts now turn into this\n",
    "count=0\n",
    "for row in train_df.itertuples():\n",
    "    print(\"Text:\", row[2])\n",
    "    print('processed_text:', row[4],\"\\n\")\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Training\n",
    "1. The dataset is further separated into training and testing subsets from the original trainning data, with five percent reserved for testing. \n",
    "\n",
    "2. After the data is partitioned, I loaded the pretrained Google News Word2Vec model, which provides 300-dimensional word embeddings derived from large-scale English corpora. To integrate these embeddings into the model, a function is defined to convert each text sample into a single vector. This function tokenizes the input sentence, identifies the words that exist in the Word2Vec vocabulary, retrieves their corresponding vectors, and then averages them to generate a fixed-length representation of the entire sentence. If no valid tokens are identified, a zero vector is used to maintain dimensional consistency.\n",
    "\n",
    "3. Then all training and testing text entries are transformed into numerical embeddings. Each sample becomes a dense vector that can be directly fed into a machine-learning model. Since neural networks cannot directly interpret string labels such as “sadness” or “joy,”  the emotion categories are encoded into numerical form using a label encoder. This ensures a consistent mapping across both training and evaluation stages.\n",
    "\n",
    "4. The model is then constructed using a sequential neural-network architecture. The network includes 2 dense layers with ReLU activation functions, a dropout layer of 0.5 designed to mitigate overfitting, and a softmax output layer that produces emotion probabilities for the text. The model is compiled with the Adam optimizer and trained using sparse categorical cross-entropy.\n",
    "\n",
    "5. To support stability during training, the script incorporates early stopping and adaptive learning-rate scheduling. Early stopping monitors the validation loss and halts training when improvements stop occurring, while the learning-rate scheduler reduces the learning rate when progress plateaus. These techniques enhance the efficiency of the training process and reduce the likelihood of overfitting.\n",
    "\n",
    "6. The model is then trained on the vectorized dataset, with a portion of the training data allocated for validation. Through iterative optimization, the network learns to associate the Word2Vec-derived sentence embeddings with their respective emotion categories. Finally, the model is evaluated on the previously isolated test set, yielding a test loss and test accuracy that reflect its generalization capability on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 45495\n",
      "Test size : 2395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "\n",
    "\n",
    "\n",
    "X_data = train_df[\"processed_text\"].astype(str).values\n",
    "y_data = train_df[\"emotion\"].values\n",
    "\n",
    "# 切 train / test（這裡 5% 當 test）\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_data,\n",
    "    y_data,\n",
    "    test_size=0.05,\n",
    "    random_state=0,\n",
    "    stratify=y_data  # 依照情緒比例分層抽樣\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_text))\n",
    "print(\"Test size :\", len(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 300\n",
      "X_train shape: (45495, 300)\n",
      "X_test shape : (2395, 300)\n"
     ]
    }
   ],
   "source": [
    "# 載入 Pretrained Google_news Word2Vec 模型\n",
    "\n",
    "w2v_path = \"dm-lab-2-private-competition/GoogleNews-vectors-negative_300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "embedding_dim = w2v.vector_size  \n",
    "print(\"Embedding dim:\", embedding_dim)\n",
    "\n",
    "#把一則文字轉成平均的 Word2Vec 向量\n",
    "def text_to_vec(text, model=w2v, embedding_dim=embedding_dim):\n",
    "    \"\"\"把一則文字轉成平均的 Word2Vec 向量\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    vecs = [model[w] for w in tokens if w in model.key_to_index]\n",
    "    \n",
    "    if not vecs:\n",
    "        # 如果裡面沒有任何在詞向量中的字，就給 0 向量\n",
    "        return np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    \n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "#  將文字轉成向量 \n",
    "\n",
    "X_train = np.vstack([text_to_vec(t) for t in X_train_text])\n",
    "X_test  = np.vstack([text_to_vec(t) for t in X_test_text])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_train, embedding_dim)\n",
    "print(\"X_test shape :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "num_classes: 6\n"
     ]
    }
   ],
   "source": [
    "# Python can't understand emotions like \"sadness\" or \"joy\", so here we have to change all 6 emotions into a numeric category of 0~6\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m77,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,726</span> (432.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,726\u001b[0m (432.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential([\n",
    "    Input(shape=(embedding_dim,)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\"),  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y 是整數 label，所以用 sparse\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "rlr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - accuracy: 0.6039 - loss: 1.0566 - val_accuracy: 0.5888 - val_loss: 1.0786 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - accuracy: 0.6100 - loss: 1.0369 - val_accuracy: 0.5895 - val_loss: 1.0857 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754us/step - accuracy: 0.6174 - loss: 1.0176 - val_accuracy: 0.5943 - val_loss: 1.0738 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step - accuracy: 0.6253 - loss: 0.9958 - val_accuracy: 0.5949 - val_loss: 1.0734 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - accuracy: 0.6328 - loss: 0.9709 - val_accuracy: 0.5921 - val_loss: 1.0796 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1223/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.6446 - loss: 0.9348\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.6434 - loss: 0.9470 - val_accuracy: 0.5947 - val_loss: 1.0848 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - accuracy: 0.6618 - loss: 0.8963 - val_accuracy: 0.5967 - val_loss: 1.1068 - learning_rate: 5.0000e-04\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - accuracy: 0.6117 - loss: 1.0273\n",
      "Test loss: 1.0273  |  Test acc: 0.6117\n"
     ]
    }
   ],
   "source": [
    "# Input training data\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=[es, rlr],\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_loss:.4f}  |  Test acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4 Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/nxc83svx6bnczl6crz71hrkw0000gn/T/ipykernel_28890/3121004903.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>ident</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we got the ranch  loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>and that got my head bobbing a little bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>same  glad it is not just out store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>like always i will wait and see thanks for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>thereis a bit of room between  not loving sub ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>we all do it sometimes do not worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>this new year i visited more relatives than us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>r u a dad or did ur dad leave u both have bad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>pre prepare direction plays hale and hearty si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id                                               text hashtags  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...       []   \n",
       "4      0xaba820         and that got my head bobbing a little bit.       []   \n",
       "5      0x66e44d                Same. Glad it's not just out store.       []   \n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...       []   \n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...       []   \n",
       "...         ...                                                ...      ...   \n",
       "64146  0x0f273c                We all do it sometimes don't worry.       []   \n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...       []   \n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...       []   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...       []   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...       []   \n",
       "\n",
       "      emotion ident                                     processed_text  \n",
       "0         NaN  test  we got the ranch  loaded our guns and sat up t...  \n",
       "4         NaN  test         and that got my head bobbing a little bit   \n",
       "5         NaN  test               same  glad it is not just out store   \n",
       "6         NaN  test  like always i will wait and see thanks for the...  \n",
       "8         NaN  test  thereis a bit of room between  not loving sub ...  \n",
       "...       ...   ...                                                ...  \n",
       "64146     NaN  test               we all do it sometimes do not worry   \n",
       "64150     NaN  test  this new year i visited more relatives than us...  \n",
       "64157     NaN  test  r u a dad or did ur dad leave u both have bad ...  \n",
       "64168     NaN  test  i got my first raspberry from a crowd surfer f...  \n",
       "64170     NaN  test  pre prepare direction plays hale and hearty si...  \n",
       "\n",
       "[16281 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unclassified text data\n",
    "test_df['processed_text'] = test_df.text.apply(preprocess_apply)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.22979933e-01, 3.68012721e-03, 2.64608636e-02, 5.12456417e-01,\n",
       "        7.52074877e-03, 2.26901829e-01],\n",
       "       [5.79092763e-02, 5.67220151e-03, 6.81349695e-01, 1.44356295e-01,\n",
       "        3.10226120e-02, 7.96900317e-02],\n",
       "       [9.01271924e-02, 8.57952144e-03, 7.43259070e-03, 6.33115232e-01,\n",
       "        1.22691981e-01, 1.38053477e-01],\n",
       "       ...,\n",
       "       [2.22188517e-01, 3.17916125e-02, 8.08604062e-03, 4.45347011e-01,\n",
       "        2.19614059e-01, 7.29727298e-02],\n",
       "       [3.13539892e-01, 1.63444970e-02, 3.00939441e-01, 3.13508034e-01,\n",
       "        4.22553048e-02, 1.34128630e-02],\n",
       "       [1.01556545e-02, 1.40085549e-05, 1.01512715e-05, 9.83254015e-01,\n",
       "        9.00041414e-05, 6.47614058e-03]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = np.vstack([text_to_vec(t) for t in test_df['processed_text']])\n",
    "pred_result= model.predict(predict)\n",
    "pred_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16281, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Each row has 6 numbers representing possible emotions. \n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "pred_result.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>Same. Glad it's not just out store.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>Like always i will wait and see thanks for the...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>There's a bit of room between \"not loving sub-...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>0x0f273c</td>\n",
       "      <td>We all do it sometimes don't worry.</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64150</th>\n",
       "      <td>0xfc4c5d</td>\n",
       "      <td>This New Year I visited more relatives than us...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64157</th>\n",
       "      <td>0xb318a3</td>\n",
       "      <td>R u a dad or did ur dad leave u both have bad ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Text_id                                               text emotion\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...     joy\n",
       "4      0xaba820         and that got my head bobbing a little bit.    fear\n",
       "5      0x66e44d                Same. Glad it's not just out store.     joy\n",
       "6      0xc03cf5  Like always i will wait and see thanks for the...     joy\n",
       "8      0x02f65a  There's a bit of room between \"not loving sub-...     joy\n",
       "...         ...                                                ...     ...\n",
       "64146  0x0f273c                We all do it sometimes don't worry.     joy\n",
       "64150  0xfc4c5d  This New Year I visited more relatives than us...   anger\n",
       "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...     joy\n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   anger\n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...     joy\n",
       "\n",
       "[16281 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_class_idx = np.argmax(pred_result, axis=1) # For each text choose the most likely emotion\n",
    "pred_labels = label_encoder.inverse_transform(pred_class_idx) # Changing the numeric emotion category back to words.\n",
    "result_df = pd.DataFrame({\"Text_id\" : test_df[\"post_id\"],\"text\": test_df[\"text\"], \"emotion\" :pred_labels})\n",
    "result_df\n",
    "# From the train we learnt that this model has a 62% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python\n(dm2025lab2)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
